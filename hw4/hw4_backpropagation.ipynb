{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw4_backpropagation.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6chybAVFJW2"
   },
   "source": [
    "# **Notebook 4: Backpropagation**\n",
    "\n",
    "In this notebook we will implement a fully connected neural network manually and\n",
    "then complete forward and backward pass functions. We'll then put it all together\n",
    "in a training loop.\n",
    "\n",
    "We'll follow the steps in section 7.4 of the book.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\" or incomplete\n",
    "lines that end with `...`. Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the notebook on Colab\n",
    "\n",
    "To complete the notebook on Colab, you can click on the \"Open in Colab\" button\n",
    "below and it will open the notebook from our public class notebook repository\n",
    "on GitHub.\n",
    "\n",
    "> Note that you will have save the notebook to your own Google Drive by clicking\n",
    "on File -> Save a Copy in Drive.\n",
    "\n",
    "> Also note that the Otter Grader cells will not run in Colab, but you can just\n",
    "> avoid executing those cells. For any public tests, you can submit to Gradescope\n",
    "> and inspect your results there.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2025_homeworks/blob/main/hw4/hw4_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Statement of AI Use and Correction Reflection (if applicable)\n",
    "\n",
    "You may use ChatGPT/Generative AI as a resource to help you complete the assignment. \n",
    "However, it must be used constructively to help you understand things you are \n",
    "unsure of, and be built upon with original code. \n",
    "\n",
    "You must cite your interaction \n",
    "by describing your prompt and the corresponding response. \n",
    "In addition, you must  explain all code from the AI that you implement in your \n",
    "assignment with inline comments. \n",
    "Touch upon how the code works and how it helped you. Failure to do \n",
    "so could result in credit deduction. \n",
    "\n",
    "The official GAIA Policy can be found here: https://www.bu.edu/cds-faculty/culture-community/gaia-policy/ \n",
    "\n",
    "Moreover, if this is a correction submission after the initial submission, \n",
    "you must provide a reflection on what you learned from the initial submission \n",
    "and how you corrected it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdIDglk1FFcG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple network\n",
    "\n",
    "Define the number of hidden layers, neurons per layer, input layer, and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVM4Tc_jGI0Q"
   },
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "K = 5\n",
    "\n",
    "# Number of neurons per layer\n",
    "D = 6\n",
    "\n",
    "# Input layer\n",
    "D_i = 1\n",
    "\n",
    "# Output layer\n",
    "D_o = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnUoI0m6GyjC"
   },
   "source": [
    "Create all the weight matrices and bias vectors for the dimensions we defined,\n",
    "initialized with random (normally distributed) numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we always get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "#   Create a network with K hidden layers, D neurons per layer, D_i input neurons,\n",
    "#  and D_o output neurons. Normalize the weights and biases to be standard normal.\n",
    "def create_network(K, D, D_i, D_o):\n",
    "  # Make empty lists\n",
    "  all_weights = [None] * (K+1)\n",
    "  all_biases = [None] * (K+1)\n",
    "\n",
    "  # Create input and output layers\n",
    "  all_weights[0] = np.random.normal(size=(D, D_i))\n",
    "  all_weights[-1] = np.random.normal(size=(D_o, D))\n",
    "  all_biases[0] = np.random.normal(size =(D,1))\n",
    "  all_biases[-1]= np.random.normal(size =(D_o,1))\n",
    "\n",
    "  # Create intermediate layers\n",
    "  for layer in range(1,K):\n",
    "    all_weights[layer] = np.random.normal(size=(D,D))\n",
    "    all_biases[layer] = np.random.normal(size=(D,1))\n",
    "\n",
    "  return all_weights, all_biases\n",
    "\n",
    "all_weights, all_biases = create_network(K, D, D_i, D_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the shapes of the weights and biases for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer shapes:\")\n",
    "for i, (weights, biases) in enumerate(zip(all_weights, all_biases)):\n",
    "    print(f\"Layer {i}: weights shape = {weights.shape}, biases shape = {biases.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZh-7bPXIDq4"
   },
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation: np.ndarray) -> np.ndarray:\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5irtyxnLJSGX"
   },
   "source": [
    "Now let's complete the forward pass function for our random network and run it.\n",
    "\n",
    "The weight matrices $\\boldsymbol\\Omega_{0\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{0\\ldots K}$ are the entries of the list \"all_biases\"\n",
    "\n",
    "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Complete the code below to implement the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgquJUJvJPaN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the forward pass of the network.\n",
    "# The input, net_input, is a matrix of shape (D_i, N) where N is the number of samples,\n",
    "# which means that when N>1, we'll get N output samples from the network as well.\n",
    "# We also return the preactivations and activations at each layer as lists of matrices.\n",
    "# We also return the preactivations and activations at each layer as lists of matrices.\n",
    "def forward_pass(net_input, all_weights, all_biases):\n",
    "\n",
    "  # Retrieve number of layers\n",
    "  K = len(all_weights) -1\n",
    "\n",
    "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
    "  # and the activations in a second list \"all_h\".\n",
    "  all_f = [None] * (K+1)\n",
    "  all_h = [None] * (K+1)\n",
    "\n",
    "  #For convenience, we'll set\n",
    "  # all_h[0] to be the input, and all_f[K] will be the output\n",
    "  all_h[0] = net_input\n",
    "\n",
    "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
    "  for layer in range(K):\n",
    "      # Update preactivations and activations at this layer according to eqn 7.17\n",
    "      # Remember to use np.matmul for matrix multiplications\n",
    "      # TODO -- Complete the lines below\n",
    "      all_f[layer] = ...\n",
    "      all_h[layer+1] = ...\n",
    "\n",
    "  # Compute the output from the last hidden layer\n",
    "  # TODO -- Complete the line below\n",
    "  all_f[K] = ...\n",
    "\n",
    "  # Retrieve the output\n",
    "  net_output = all_f[K]\n",
    "\n",
    "  return net_output, all_f, all_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input\n",
    "net_input = np.ones((D_i,1)) * 1.2\n",
    "\n",
    "# Compute network output\n",
    "net_output, all_f, all_h = forward_pass(net_input,all_weights, all_biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Preactivation and activation shapes:\")\n",
    "\n",
    "def print_activations(all_f, all_h):\n",
    "    for i, (f, h) in enumerate(zip(all_f, all_h)):\n",
    "        if i == 0:\n",
    "            print(f\"h_0 shape = {h.shape}\\n\")\n",
    "            print(f\"f_0 shape = {f.shape}\", end=\" \")\n",
    "        else:\n",
    "            print(f\"h_{i} shape = {h.shape}\\n\")\n",
    "            print(f\"f_{i} shape = {f.shape}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "print_activations(all_f, all_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input\n",
    "net_input = np.ones((D_i,1)) * 1.2\n",
    "\n",
    "# Compute network output\n",
    "net_output, all_f, all_h = forward_pass(net_input,all_weights, all_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxVTKp3IcoBF"
   },
   "source": [
    "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute \n",
    "$\\partial l / \\partial \\mathbf{f}_K$, e.g. the derivative of the loss with respect to the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the network again\n",
    "\n",
    "# Set seed so we always get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define network dimensions back to 1-D input\n",
    "K = 5; D = 6; D_i = 1; D_o = 1\n",
    "\n",
    "# Create and initialize the network\n",
    "all_weights, all_biases = create_network(K, D, D_i, D_o)\n",
    "\n",
    "# Define input\n",
    "net_input = np.ones((D_i,1)) * 1.2\n",
    "print(\"net_input.shape\", net_input.shape)\n",
    "\n",
    "# Compute the forward pass\n",
    "net_output, all_f, all_h = forward_pass(net_input,all_weights, all_biases)\n",
    "print(\"net_output\", net_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XqWSYWJdhQR"
   },
   "outputs": [],
   "source": [
    "def least_squares_loss(net_output: np.ndarray, y: np.ndarray) -> float:\n",
    "  return np.sum((net_output-y) * (net_output-y))\n",
    "\n",
    "def d_loss_d_output(net_output, y):\n",
    "    return 2*(net_output - y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njF2DUQmfttR"
   },
   "outputs": [],
   "source": [
    "y = np.ones((D_o,1)) * 20.0\n",
    "loss = least_squares_loss(net_output, y)\n",
    "print(f\"y = {y} Loss = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98WmyqFYWA-0"
   },
   "source": [
    "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Complete the code below to implement the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJng7WpRPLMz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll need the indicator function\n",
    "def indicator_function(x):\n",
    "  x_in = np.array(x)\n",
    "  x_in[x_in>0] = 1\n",
    "  x_in[x_in<=0] = 0\n",
    "  return x_in\n",
    "\n",
    "# Main backward pass routine\n",
    "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
    "\n",
    "  # Retrieve number of layers\n",
    "  K = len(all_weights) -1\n",
    "\n",
    "  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
    "  all_dl_dweights = [None] * (K+1)\n",
    "  all_dl_dbiases = [None] * (K+1)\n",
    "  \n",
    "  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n",
    "  all_dl_df = [None] * (K+1)\n",
    "  all_dl_dh = [None] * (K+1)\n",
    "  \n",
    "  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
    "\n",
    "  # Compute derivatives of the loss with respect to the network output\n",
    "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
    "\n",
    "  # Now work backwards through the network, Follow from equations 7.22 onwards\n",
    "  for layer in range(K,-1,-1):\n",
    "\n",
    "    # TODO Calculate the derivatives of the loss with respect to the biases at layer from all_dl_df[layer]. (eq 7.22)\n",
    "    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
    "    # COMPLETE THIS LINE\n",
    "    all_dl_dbiases[layer] = ...\n",
    "\n",
    "    # TODO Calculate the derivatives of the loss with respect to the weights at layer from all_dl_df[layer] and \n",
    "    # all_h[layer] (eq 7.23)\n",
    "    # Don't forget to use np.matmul\n",
    "    # COMPLETE THIS LINE\n",
    "    all_dl_dweights[layer] = ...\n",
    "\n",
    "    # TODO: calculate the derivatives of the loss with respect to the activations from weight and derivatives of next\n",
    "    # preactivations (second part of last line of eq 7.25)\n",
    "    # COMPLETE THIS LINE\n",
    "    all_dl_dh[layer] = ...\n",
    "\n",
    "\n",
    "    if layer > 0:\n",
    "      # TODO Calculate the derivatives of the loss with respect to the pre-activation f (use derivative of ReLu function,\n",
    "      # first part of last line of eq. 7.25)\n",
    "      # COMPLETE THIS LINE\n",
    "      all_dl_df[layer-1] = ...\n",
    "\n",
    "  return all_dl_dweights, all_dl_dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A9MHc4sQvbp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK-UtE3hreAK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "# Make space for derivatives computed by finite differences\n",
    "all_dl_dweights_fd = [None] * (K+1)\n",
    "all_dl_dbiases_fd = [None] * (K+1)\n",
    "\n",
    "# Let's test if we have the derivatives right using finite differences\n",
    "delta_fd = 0.000001\n",
    "\n",
    "# Test the dervatives of the bias vectors\n",
    "for layer in range(K+1):\n",
    "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
    "  # For every element in the bias\n",
    "  for row in range(all_biases[layer].shape[0]):\n",
    "    # Take copy of biases  We'll change one element each time\n",
    "    all_biases_copy = [np.array(x) for x in all_biases]\n",
    "    all_biases_copy[layer][row] += delta_fd\n",
    "    network_output_1, *_ = forward_pass(net_input, all_weights, all_biases_copy)\n",
    "    network_output_2, *_ = forward_pass(net_input, all_weights, all_biases)\n",
    "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dbiases[layer])\n",
    "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dbiases_fd[layer])\n",
    "  assert np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False), \"Derivatives do not match\"\n",
    "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")\n",
    "\n",
    "\n",
    "\n",
    "# Test the derivatives of the weights matrices\n",
    "for layer in range(K+1):\n",
    "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
    "  # For every element in the bias\n",
    "  for row in range(all_weights[layer].shape[0]):\n",
    "    for col in range(all_weights[layer].shape[1]):\n",
    "      # Take copy of biases  We'll change one element each time\n",
    "      all_weights_copy = [np.array(x) for x in all_weights]\n",
    "      all_weights_copy[layer][row][col] += delta_fd\n",
    "      network_output_1, *_ = forward_pass(net_input, all_weights_copy, all_biases)\n",
    "      network_output_2, *_ = forward_pass(net_input, all_weights, all_biases)\n",
    "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dweights[layer])\n",
    "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dweights_fd[layer])\n",
    "  assert np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False), \"Derivatives do not match\"\n",
    "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "Now we have all the pieces we need to implement a training loop. We'll define\n",
    "a trivial data set of 4 samples, and then construct a training loop and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "Define our training set to be 4 samples, 3 dimensional input, and 1 dimensional output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = np.array([\n",
    "        [2.0, 3.0, -1.0],\n",
    "        [3.0, -1.0, 0.5],\n",
    "        [0.5, 1.0, 1.0],\n",
    "        [1.0, 1.0, -1.0]\n",
    "    ])\n",
    "net_input = net_input.T\n",
    "print(\"net_input\", net_input)\n",
    "print(\"net_input.shape\", net_input.shape)\n",
    "\n",
    "y = np.array([1.0, 0.0, 0.0, 1.0])\n",
    "print(\"y\", y)\n",
    "print(\"y.shape\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and initialize the network\n",
    "\n",
    "Define the dimensions of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "K = 5\n",
    "# Number of neurons per layer\n",
    "D = 6\n",
    "# Input layer\n",
    "D_i = 3\n",
    "# Output layer\n",
    "D_o = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and initialize the appropriate number of weights and biases for each layer.\n",
    "\n",
    "We randomly initialize each to a standard normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we always get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "all_weights, all_biases = create_network(K, D, D_i, D_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of the weights and biases for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer shapes:\")\n",
    "for i, (weights, biases) in enumerate(zip(all_weights, all_biases)):\n",
    "    print(f\"Layer {i}: weights shape = {weights.shape}, biases shape = {biases.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the forward pass. Because of the broadcasting rules of numpy, \n",
    "we can give all the inputs as one matrix and get the output for all the inputs at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200\n",
    "alpha = 0.01   # learning rate\n",
    "\n",
    "losses = []\n",
    "for i in range(num_iterations):\n",
    "    net_output, all_f, all_h = forward_pass(net_input,all_weights, all_biases)\n",
    "    y_pred = net_output\n",
    "    loss = least_squares_loss(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    print(f\"Iteration {i+1}: Loss = {loss:.3f}\")\n",
    "\n",
    "    all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
    "\n",
    "    for i, (weights, biases) in enumerate(zip(all_weights, all_biases)):\n",
    "        all_weights[i] = weights - alpha * all_dl_dweights[i]\n",
    "        all_biases[i] = biases - alpha * all_dl_dbiases[i]\n",
    "\n",
    "print(y)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss over the iterations.  We'll plot the full loss over the iterations, and then zoom in on the loss from iteration 10 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Full plot\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss over Iterations')\n",
    "\n",
    "# Plot starting from iteration 10\n",
    "ax2.plot(range(10, len(losses)), losses[10:])\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss from Iteration 10')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3:**\n",
    "\n",
    "What do you notice about how the loss progresses over the iterations? Is it a gradual decrease or a sharp decrease? To what do you attribute the rate of\n",
    "decrease in the first few iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Build and Train an Equivalent Network with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we'll now define the network in PyTorch and train it with the\n",
    "same tiny dataset. Hopefully you see some similarities in how the network is\n",
    "defined and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Manually seed the Random Number Generator for Reproducibility\n",
    "# You can comment the next line out see the variability\n",
    "torch.manual_seed(99)\n",
    "\n",
    "# Step 1: Define the MLP model\n",
    "class ptMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ptMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = ptMLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a loss function and an optimizer\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "# Step 3: Create a tiny dataset\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# we had to transpose ys for torch.tensor\n",
    "ys_transpose = [[1.0], \n",
    "      [0.0], \n",
    "      [0.0], \n",
    "      [1.0]]\n",
    "\n",
    "inputs = torch.tensor(xs)\n",
    "outputs = torch.tensor(ys_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Write the training loop\n",
    "losses = []\n",
    "niters = 300\n",
    "\n",
    "for epoch in range(niters):\n",
    "\n",
    "    # Training Step 1: Forward pass\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    # Training Step 2: Calculate the loss\n",
    "    loss = criterion(predictions, outputs)\n",
    "\n",
    "    # Training Step 3: Zero the gradient and run backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Training Step 4: Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    # print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(f'Final Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Loss Per Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the predictions and the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(outputs, [y.item() for y in predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "Compare the loss curves and number of iterations between the PyTorch implemenation  and the manual implementation.\n",
    "\n",
    "What differences do you see?\n",
    "\n",
    "To what do you attribute the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<b>The End</b>\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl4ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw4_backpropagation",
   "tests": {
    "q1": {
     "name": "q1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(net_output[0, 0], 1.907, atol=0.001), 'Incorrect output'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> K = 5\n>>> D = 6\n>>> D_i = 3\n>>> D_o = 1\n>>> np.random.seed(42)\n>>> all_weights, all_biases = create_network(K, D, D_i, D_o)\n>>> net_input = np.ones((D_i, 4)) * 1.9\n>>> net_output, all_f, all_h = forward_pass(net_input, all_weights, all_biases)\n>>> assert np.allclose(net_output, np.array([[-2.226, -2.226, -2.226, -2.226]]), atol=0.001), 'Incorrect output'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.allclose(all_dl_dbiases_fd[layer], all_dl_dbiases[layer], rtol=1e-05, atol=1e-08, equal_nan=False), 'Derivatives do not match'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.allclose(all_dl_dweights_fd[layer], all_dl_dweights[layer], rtol=1e-05, atol=1e-08, equal_nan=False), 'Derivatives do not match'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
