{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw3_loss_functions_and_fitting_models.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the notebook on Colab\n",
    "\n",
    "To complete the notebook on Colab, you can click on the \"Open in Colab\" button\n",
    "below and it will open the notebook from our public class notebook repository\n",
    "on GitHub.\n",
    "\n",
    "> Note that you will have save the notebook to your own Google Drive by clicking\n",
    "on File -> Save a Copy in Drive.\n",
    "\n",
    "> Also note that the Otter Grader cells will not run in Colab, but you can just\n",
    "> avoid executing those cells. For any public tests, you can submit to Gradescope\n",
    "> and inspect your results there.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2025_homeworks/blob/main/hw3/hw3_loss_functions_and_fitting_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the notebook in a local environment\n",
    "\n",
    "For local environments, after setting up your Python and Jupyter environments,\n",
    "you have the option of using either\n",
    "* Jupyter Lab or Notebook interface, or with\n",
    "* VSCode and the Jupyter extension pack\n",
    "\n",
    "In both cases we strongly encourage you to make sure you have a recent version\n",
    "of Python (>=3.10) and use a virtual environment such as `venv` or miniconda\n",
    "environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.1 Multiclass Cross-Entropy Loss**\n",
    "\n",
    "The following cells investigates the multi-class cross-entropy loss. \n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Used for repmat\n",
    "import numpy.matlib\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# Import math Library\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation\n",
    "\n",
    "# Define a shallow neural network\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
    "    # Make sure that input data is (1 x n_data) array\n",
    "    n_data = x.size\n",
    "    x = np.reshape(x,(1,n_data))\n",
    "\n",
    "    # This runs the network for ALL of the inputs, x at once so we can draw graph\n",
    "    h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(omega_0,x))\n",
    "    model_out = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(omega_1,h1)\n",
    "    return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for model -- we can call this function to easily reset them\n",
    "def get_parameters():\n",
    "  # And we'll create a network that approximately fits it\n",
    "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
    "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
    "  beta_1 = np.zeros((3,1));  # NOTE -- there are three outputs now (one for each class, so three output biases)\n",
    "  omega_1 = np.zeros((3,3)); # NOTE -- there are three outputs now (one for each class, so nine output weights, connecting 3 hidden units to 3 outputs)\n",
    "\n",
    "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
    "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
    "  beta_1[0,0] = 2.0; beta_1[1,0] = -2; beta_1[2,0] = 0.0\n",
    "  omega_1[0,0] = -24.0; omega_1[0,1] = -8.0; omega_1[0,2] = 50.0\n",
    "  omega_1[1,0] = -2.0; omega_1[1,1] = 8.0; omega_1[1,2] = -30.0\n",
    "  omega_1[2,0] = 16.0; omega_1[2,1] = -8.0; omega_1[2,2] =-8\n",
    "\n",
    "  return beta_0, omega_0, beta_1, omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for plotting data\n",
    "def plot_multiclass_classification(x_model, out_model, lambda_model, x_data = None, y_data = None, title= None):\n",
    "  # Make sure model data are 1D arrays\n",
    "  n_data = len(x_model)\n",
    "  n_class = 3\n",
    "  x_model = np.squeeze(x_model)\n",
    "  out_model = np.reshape(out_model, (n_class,n_data))\n",
    "  lambda_model = np.reshape(lambda_model, (n_class,n_data))\n",
    "\n",
    "  fig, ax = plt.subplots(1,2)\n",
    "  fig.set_size_inches(7.0, 3.5)\n",
    "  fig.tight_layout(pad=3.0)\n",
    "  ax[0].plot(x_model,out_model[0,:],'r-')\n",
    "  ax[0].plot(x_model,out_model[1,:],'g-')\n",
    "  ax[0].plot(x_model,out_model[2,:],'b-')\n",
    "  ax[0].set_xlabel('Input, $x$'); ax[0].set_ylabel('Model outputs')\n",
    "  ax[0].set_xlim([0,1]);ax[0].set_ylim([-4,4])\n",
    "  if title is not None:\n",
    "    ax[0].set_title(title)\n",
    "  ax[1].plot(x_model,lambda_model[0,:],'r-')\n",
    "  ax[1].plot(x_model,lambda_model[1,:],'g-')\n",
    "  ax[1].plot(x_model,lambda_model[2,:],'b-')\n",
    "  ax[1].set_xlabel('Input, $x$'); ax[1].set_ylabel('$\\lambda$ or Pr(y=k|x)')\n",
    "  ax[1].set_xlim([0,1]);ax[1].set_ylim([-0.1,1.05])\n",
    "  if title is not None:\n",
    "    ax[1].set_title(title)\n",
    "  if x_data is not None:\n",
    "    for i in range(len(x_data)):\n",
    "      if y_data[i] ==0:\n",
    "        ax[1].plot(x_data[i],-0.05, 'r.')\n",
    "      if y_data[i] ==1:\n",
    "        ax[1].plot(x_data[i],-0.05, 'g.')\n",
    "      if y_data[i] ==2:\n",
    "        ax[1].plot(x_data[i],-0.05, 'b.')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "For multiclass classification, the network must predict the probability of $K$ classes, using $K$ outputs.  However, these probability must be non-negative and sum to one, and the network outputs can take arbitrary values.  Hence, we pass the outputs through a softmax function which maps $K$ arbitrary values to $K$ non-negative values that sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Compute the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Softmax function that maps a vector of arbitrary values to a vector of values that are positive and sum to one.\n",
    "def softmax(model_out):\n",
    "  # Compute the exponential of the model outputs\n",
    "  exp_model_out = ...\n",
    "  # Compute the sum of the exponentials (denominator of equation 5.22)\n",
    "  sum_exp_model_out = ...\n",
    "  # Normalize the exponentials (np.matlib.repmat might be useful here)\n",
    "  softmax_model_out = ...\n",
    "\n",
    "  return softmax_model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create some 1D training data\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([2,0,1,2,1,0,\\\n",
    "                    0,2,2,0,2,0,\\\n",
    "                    2,0,1,2,1,2, \\\n",
    "                    1,0])\n",
    "\n",
    "# Get parameters for the model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "\n",
    "# Define a range of input values\n",
    "x_model = np.arange(0,1,0.01)\n",
    "# Run the model to get values to plot and plot it.\n",
    "model_out= shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_model = softmax(model_out)\n",
    "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left is model output and the right is the model output after the softmax has been applied, so it now lies in the range [0,1] and represents the probability, that y=0 (red), 1 (green) and 2 (blue).  The dots at the bottom show the training data with the same color scheme.  So we want the red curve to be high where there are red dots, the green curve to be high where there are green dots, and the blue curve to be high where there are blue dots  We'll compute the likelihood and the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return probability under categorical distribution for observed class y\n",
    "# Just take value from row k of lambda param where y =k,\n",
    "def categorical_distribution(y, lambda_param):\n",
    "    return np.array([lambda_param[row, i] for i, row in enumerate (y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are three examples\n",
    "print(categorical_distribution(np.array([[0]]),np.array([[0.2],[0.5],[0.3]])))\n",
    "print(categorical_distribution(np.array([[1]]),np.array([[0.2],[0.5],[0.3]])))\n",
    "print(categorical_distribution(np.array([[2]]),np.array([[0.2],[0.5],[0.3]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Compute likelihood scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, lambda_param):\n",
    "  # TODO -- compute the likelihood of the data \n",
    "  likelihood = ...\n",
    "\n",
    "  return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the parameters of the categorical distribution\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_train = softmax(model_out)\n",
    "# Compute the likelihood\n",
    "likelihood = compute_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000000041,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
    "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
    "\n",
    "This is why we use negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Compute the negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, lambda_param):\n",
    "  nll = ...\n",
    "\n",
    "  return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the parameters of the categorical distribution\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Pass the outputs through the softmax function\n",
    "lambda_train = softmax(model_out)\n",
    "# Compute the negative log likelihood\n",
    "nll = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(17.015457867,nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now let's investigate finding the maximum likelihood / minimum negative log likelihood solution.  For simplicity, we'll assume that all the parameters are fixed except one and look at how the likelihood and negative log likelihood change as we manipulate the last parameter.  We'll start with overall y_offset, $\\beta_1$ (formerly $\\phi_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "beta_1_vals = np.arange(-2,6.0,0.1)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods\n",
    "likelihoods = np.zeros_like(beta_1_vals)\n",
    "nlls = np.zeros_like(beta_1_vals)\n",
    "\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "for count in range(len(beta_1_vals)):\n",
    "  # Set the value for the parameter\n",
    "  beta_1[0,0] = beta_1_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  lambda_train = softmax(model_out)\n",
    "  # Compute and store the two values\n",
    "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
    "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 20 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    lambda_model = softmax(model_out)\n",
    "    plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the likelihood and negative log likelihood as a function of the value of the offset beta1\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout(pad=5.0)\n",
    "likelihood_color = 'tab:red'\n",
    "nll_color = 'tab:blue'\n",
    "\n",
    "\n",
    "ax.set_xlabel('beta_1[0, 0]')\n",
    "ax.set_ylabel('likelihood', color = likelihood_color)\n",
    "ax.plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
    "ax.tick_params(axis='y', labelcolor=likelihood_color)\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(beta_1_vals, nlls, color = nll_color)\n",
    "ax1.set_ylabel('negative log likelihood', color = nll_color)\n",
    "ax1.tick_params(axis='y', labelcolor = nll_color)\n",
    "\n",
    "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood solution\n",
    "# Let's check that:\n",
    "print(\"Maximum likelihood = %f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
    "print(\"Minimum negative log likelihood = %f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
    "\n",
    "# Plot the best model\n",
    "beta_1[0,0] = beta_1_vals[np.argmin(nlls)]\n",
    "model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_model = softmax(model_out)\n",
    "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both give the same answer. But you can see from the likelihood above that the likelihood is very small unless the parameters are almost correct.  So in practice, we would work with the negative log likelihood.<br><br>\n",
    "\n",
    "Again, to fit the full neural model we would vary all of the 16 parameters of the network in the $\\boldsymbol\\beta_{0},\\boldsymbol\\Omega_{0},\\boldsymbol\\beta_{1},\\boldsymbol\\Omega_{1}$ until we find the combination that have the maximum likelihood / minimum negative log likelihood.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.2 Adam**\n",
    "\n",
    "The following cells investigates the Adam optimizer as illustrated in figure 6.9 from the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that we wish to find the minimum of (normally would be defined implicitly by data and loss)\n",
    "def loss(phi0, phi1):\n",
    "    height = np.exp(-0.5 * (phi1 * phi1)*4.0)\n",
    "    height = height * np. exp(-0.5* (phi0-0.7) *(phi0-0.7)/4.0)\n",
    "    return 1.0-height\n",
    "\n",
    "# Compute the gradients of this function (for simplicity, I just used finite differences)\n",
    "def get_loss_gradient(phi0, phi1):\n",
    "    delta_phi = 0.00001;\n",
    "    gradient = np.zeros((2,1));\n",
    "    gradient[0] = (loss(phi0+delta_phi/2.0, phi1) - loss(phi0-delta_phi/2.0, phi1))/delta_phi\n",
    "    gradient[1] = (loss(phi0, phi1+delta_phi/2.0) - loss(phi0, phi1-delta_phi/2.0))/delta_phi\n",
    "    return gradient[:,0];\n",
    "\n",
    "# Compute the loss function at a range of values of phi0 and phi1 for plotting\n",
    "def get_loss_function_for_plot():\n",
    "  grid_values = np.arange(-1.0,1.0,0.01);\n",
    "  phi0mesh, phi1mesh = np.meshgrid(grid_values, grid_values)\n",
    "  loss_function = np.zeros((grid_values.size, grid_values.size))\n",
    "  for idphi0, phi0 in enumerate(grid_values):\n",
    "      for idphi1, phi1 in enumerate(grid_values):\n",
    "          loss_function[idphi0, idphi1] = loss(phi1,phi0)\n",
    "  return loss_function, phi0mesh, phi1mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fancy colormap\n",
    "my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
    "my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
    "r = np.floor(my_colormap_vals_dec/(256*256))\n",
    "g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
    "b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
    "my_colormap_vals = np.vstack((r,g,b)).transpose()/255.0\n",
    "my_colormap = ListedColormap(my_colormap_vals)\n",
    "\n",
    "# Plotting function\n",
    "def draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, opt_path):\n",
    "    fig = plt.figure();\n",
    "    ax = plt.axes();\n",
    "    fig.set_size_inches(7,7)\n",
    "    ax.contourf(phi0mesh, phi1mesh, loss_function, 256, cmap=my_colormap);\n",
    "    ax.contour(phi0mesh, phi1mesh, loss_function, 20, colors=['#80808080'])\n",
    "    ax.plot(opt_path[0,:], opt_path[1,:],'-', color='#a0d9d3ff')\n",
    "    ax.plot(opt_path[0,:], opt_path[1,:],'.', color='#a0d9d3ff',markersize=10)\n",
    "    ax.set_xlabel(r\"$\\phi_{0}$\")\n",
    "    ax.set_ylabel(r\"$\\phi_{1}$\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fixed step size gradient descent\n",
    "def grad_descent(start_posn, n_steps, alpha):\n",
    "    grad_path = np.zeros((2, n_steps+1));\n",
    "    grad_path[:,0] = start_posn[:,0];\n",
    "    for c_step in range(n_steps):\n",
    "        this_grad = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step]);\n",
    "        grad_path[:,c_step+1] = grad_path[:,c_step] - alpha * this_grad\n",
    "    return grad_path;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by running gradient descent with a fixed step size for this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function, phi0mesh, phi1mesh = get_loss_function_for_plot() ;\n",
    "\n",
    "start_posn = np.zeros((2,1));\n",
    "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
    "\n",
    "# Run gradient descent\n",
    "grad_path1 = grad_descent(start_posn, n_steps=200, alpha = 0.08)\n",
    "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)\n",
    "grad_path2 = grad_descent(start_posn, n_steps=40, alpha= 1.0)\n",
    "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the function changes much faster in $\\phi_1$ than in $\\phi_0$, there is no great step size to choose.  If we set the step size so that it makes sensible progress in the $\\phi_1$ direction, then it takes many iterations to converge.  If we set the step size so that we make sensible progress in the $\\phi_0$ direction, then the path oscillates in the $\\phi_1$ direction.  \n",
    "\n",
    "This motivates Adam.  At the core of Adam is the idea that we should just determine which way is downhill along each axis (i.e. left/right for $\\phi_0$ or up/down for $\\phi_1$) and move a fixed distance in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Complete the below code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def normalized_gradients(start_posn, n_steps, alpha,  epsilon=1e-20):\n",
    "    grad_path = np.zeros((2, n_steps+1));\n",
    "    grad_path[:,0] = start_posn[:,0];\n",
    "    for c_step in range(n_steps):\n",
    "        # Measure the gradient as in equation 6.13 (first line)\n",
    "        m = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step]);\n",
    "        # TODO -- compute the squared gradient as in equation 6.13 (second line)\n",
    "        # Replace this line:\n",
    "        v = ...\n",
    "\n",
    "        # TODO -- apply the update rule (equation 6.14)\n",
    "        # Replace this line:\n",
    "        grad_path[:,c_step+1] = ...\n",
    "\n",
    "    return grad_path;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out normalized gradients\n",
    "start_posn = np.zeros((2,1));\n",
    "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
    "\n",
    "# Run gradient descent\n",
    "grad_path1 = normalized_gradients(start_posn, n_steps=40, alpha = 0.08)\n",
    "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This moves towards the minimum at a sensible speed, but we never actually converge -- the solution just bounces back and forth between the last two points.  To make it converge, we add momentum to both the estimates of the gradient and the pointwise squared gradient.  We also modify the statistics by a factor that depends on the time to make sure the progress is not slow to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Complete the below code for Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def adam(start_posn, n_steps, alpha,  beta=0.9, gamma=0.99, epsilon=1e-20):\n",
    "    grad_path = np.zeros((2, n_steps+1));\n",
    "    grad_path[:,0] = start_posn[:,0];\n",
    "    m = np.zeros_like(grad_path[:,0])\n",
    "    v = np.zeros_like(grad_path[:,0])\n",
    "    for c_step in range(n_steps):\n",
    "        # Measure the gradient\n",
    "        grad = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step])\n",
    "        # TODO -- Update the momentum based gradient estimate equation 6.15 (first line)\n",
    "        # Replace this line:\n",
    "        m = ...\n",
    "\n",
    "\n",
    "        # TODO -- update the momentum based squared gradient estimate as in equation 6.15 (second line)\n",
    "        # Replace this line:\n",
    "        v = ...\n",
    "\n",
    "        # TODO -- Modify the statistics according to equation 6.16\n",
    "        # You will need the function np.power\n",
    "        # Replace these lines\n",
    "        m_tilde = ...\n",
    "        v_tilde = ...\n",
    "\n",
    "\n",
    "        # TODO -- apply the update rule (equation 6.17)\n",
    "        # Replace this line:\n",
    "        grad_path[:,c_step+1] = ...\n",
    "\n",
    "    return grad_path;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out our Adam algorithm\n",
    "start_posn = np.zeros((2,1));\n",
    "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
    "\n",
    "# Run gradient descent\n",
    "grad_path1 = adam(start_posn, n_steps=60, alpha = 0.05)\n",
    "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw3_loss_functions_and_fitting_models",
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
